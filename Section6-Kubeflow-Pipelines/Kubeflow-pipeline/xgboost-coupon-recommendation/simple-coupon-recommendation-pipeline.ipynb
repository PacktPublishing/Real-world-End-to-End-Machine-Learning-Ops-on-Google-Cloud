{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5c83f-81ab-4a7d-8947-5d2562e00d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Output,\n",
    "    OutputPath,\n",
    "    Metrics\n",
    ")\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# Component to validate the input dataset\n",
    "@component(\n",
    "    packages_to_install=[\"gcsfs\", \"pandas\", \"google-cloud-storage\"]\n",
    ")\n",
    "def validate_input_ds(\n",
    "    filename: str,\n",
    "    input_validation: OutputPath(str)\n",
    "):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logging.info(f\"Reading file: {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Update expected number of columns based on your CSV data\n",
    "    expected_num_cols = 26  # Number of columns in your CSV data\n",
    "    num_cols = len(df.columns)\n",
    "\n",
    "    logging.info(f\"Number of columns: {num_cols}\")\n",
    "\n",
    "    input_validation_value = \"true\"\n",
    "\n",
    "    if num_cols != expected_num_cols:\n",
    "        input_validation_value = \"false\"\n",
    "        logging.error(f\"Expected {expected_num_cols} columns, but found {num_cols} columns.\")\n",
    "    else:\n",
    "        # Update expected column names based on your CSV data\n",
    "        expected_col_names = [\n",
    "            'destination', 'passanger', 'weather', 'temperature', 'time', 'coupon', 'expiration',\n",
    "            'gender', 'age', 'maritalStatus', 'has_children', 'education', 'occupation', 'income',\n",
    "            'car', 'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50',\n",
    "            'toCoupon_GEQ5min', 'toCoupon_GEQ15min', 'toCoupon_GEQ25min', 'direction_same',\n",
    "            'direction_opp', 'Y'\n",
    "        ]\n",
    "\n",
    "        if set(df.columns) != set(expected_col_names):\n",
    "            input_validation_value = \"false\"\n",
    "            missing_cols = set(expected_col_names) - set(df.columns)\n",
    "            extra_cols = set(df.columns) - set(expected_col_names)\n",
    "            logging.error(f\"Column names do not match expected names.\")\n",
    "            if missing_cols:\n",
    "                logging.error(f\"Missing columns: {missing_cols}\")\n",
    "            if extra_cols:\n",
    "                logging.error(f\"Unexpected columns: {extra_cols}\")\n",
    "\n",
    "    # Write the result to the output file\n",
    "    with open(input_validation, 'w') as f:\n",
    "        f.write(input_validation_value)\n",
    "\n",
    "# Component for custom training job\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\", \"gcsfs\", \"xgboost\", \"category_encoders\",\n",
    "        \"imblearn\", \"pandas\", \"google-cloud-storage\"\n",
    "    ]\n",
    ")\n",
    "def custom_training_job_component(\n",
    "    max_depth: int,\n",
    "    learning_rate: float,\n",
    "    n_estimators: int,\n",
    "    metrics: Output[Metrics],\n",
    "    model_validation: OutputPath(str)\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from category_encoders import HashingEncoder\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from xgboost import XGBClassifier\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(\"udemy-llm-course\")\n",
    "\n",
    "    def load_data(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(df):\n",
    "        df = df.drop(columns=['car', 'toCoupon_GEQ5min', 'direction_opp'])\n",
    "        df = df.fillna(df.mode().iloc[0])\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        df_dummy = df.copy()\n",
    "        age_list = []\n",
    "        for i in df['age']:\n",
    "            if i == 'below21':\n",
    "                age = '<21'\n",
    "            elif i in ['21', '26']:\n",
    "                age = '21-30'\n",
    "            elif i in ['31', '36']:\n",
    "                age = '31-40'\n",
    "            elif i in ['41', '46']:\n",
    "                age = '41-50'\n",
    "            else:\n",
    "                age = '>50'\n",
    "            age_list.append(age)\n",
    "        df_dummy['age'] = age_list\n",
    "\n",
    "        df_dummy['passanger_destination'] = df_dummy['passanger'].astype(str) + '-' + df_dummy['destination'].astype(str)\n",
    "        df_dummy['marital_hasChildren'] = df_dummy['maritalStatus'].astype(str) + '-' + df_dummy['has_children'].astype(str)\n",
    "        df_dummy['temperature_weather'] = df_dummy['temperature'].astype(str) + '-' + df_dummy['weather'].astype(str)\n",
    "        df_dummy = df_dummy.drop(columns=['passanger', 'destination', 'maritalStatus', 'has_children', 'temperature','weather', 'Y'])\n",
    "\n",
    "        df_dummy = pd.concat([df_dummy, df['Y']], axis=1)\n",
    "        df_dummy = df_dummy.drop(columns=['gender', 'RestaurantLessThan20'])\n",
    "        df_le = df_dummy.replace({\n",
    "            'expiration':{'2h': 0, '1d' : 1},\n",
    "            'age':{'<21': 0, '21-30': 1, '31-40': 2, '41-50': 3, '>50': 4},\n",
    "            'education':{'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2,\n",
    "                         'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5},\n",
    "            'Bar':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n",
    "            'CoffeeHouse':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n",
    "            'CarryAway':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n",
    "            'Restaurant20To50':{'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4},\n",
    "            'income':{'Less than $12500':0, '$12500 - $24999':1, '$25000 - $37499':2, '$37500 - $49999':3,\n",
    "                      '$50000 - $62499':4, '$62500 - $74999':5, '$75000 - $87499':6, '$87500 - $99999':7,\n",
    "                      '$100000 or More':8},\n",
    "            'time':{'7AM':0, '10AM':1, '2PM':2, '6PM':3, '10PM':4}\n",
    "        })\n",
    "\n",
    "        x = df_le.drop('Y', axis=1)\n",
    "        y = df_le.Y\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def train_model(x_train, y_train, max_depth, learning_rate, n_estimators):\n",
    "        model = XGBClassifier(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(model, x_test, y_test):\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "\n",
    "        return accuracy, precision, recall\n",
    "\n",
    "    def encode_features(x, n_components=27):\n",
    "        hashing_ros_enc = HashingEncoder(\n",
    "            cols=['passanger_destination', 'marital_hasChildren', 'occupation', 'coupon', 'temperature_weather'],\n",
    "            n_components=n_components\n",
    "        ).fit(x)\n",
    "        x_hashing = hashing_ros_enc.transform(x.reset_index(drop=True))\n",
    "        return x_hashing\n",
    "\n",
    "    def oversample_data(x_train_hashing, y_train):\n",
    "        sm = SMOTE(random_state=42)\n",
    "        x_sm_train_hashing, y_sm_train = sm.fit_resample(x_train_hashing, y_train)\n",
    "        return x_sm_train_hashing, y_sm_train\n",
    "\n",
    "    def save_model_artifact(model):\n",
    "        artifact_name = 'model.bst'\n",
    "        model.save_model(artifact_name)\n",
    "        model_artifact = bucket.blob('coupon-recommendation/artifacts/' + artifact_name)\n",
    "        model_artifact.upload_from_filename(artifact_name)\n",
    "\n",
    "    input_file = \"gs://bucket-name/in-vehicle-coupon-recommendation.csv\"\n",
    "    df = load_data(input_file)\n",
    "    x, y = preprocess_data(df)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    x_train.fillna(x_train.mode().iloc[0], inplace=True)\n",
    "    x_test.fillna(x_train.mode().iloc[0], inplace=True)\n",
    "\n",
    "    x_train_hashing = encode_features(x_train)\n",
    "    x_test_hashing = encode_features(x_test)\n",
    "    x_sm_train_hashing, y_sm_train = oversample_data(x_train_hashing, y_train)\n",
    "\n",
    "    model = train_model(x_sm_train_hashing, y_sm_train, max_depth, learning_rate, n_estimators)\n",
    "\n",
    "    accuracy, precision, recall = evaluate_model(model, x_test_hashing, y_test)\n",
    "    metrics.log_metric(\"accuracy\", accuracy)\n",
    "    metrics.log_metric(\"precision\", precision)\n",
    "    metrics.log_metric(\"recall\", recall)\n",
    "\n",
    "    # Decide whether to save the model based on evaluation metrics\n",
    "    model_validation_value = \"true\"\n",
    "    if accuracy > 0.5 and precision > 0.5 and recall > 0.5:\n",
    "        save_model_artifact(model)\n",
    "        model_validation_value = \"true\"\n",
    "    else:\n",
    "        model_validation_value = \"false\"\n",
    "\n",
    "    # Write the model validation result to the output path\n",
    "    with open(model_validation, 'w') as f:\n",
    "        f.write(model_validation_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3188cb-34d7-48aa-a51b-489cb0a1ca75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=\"gs://bucket-name/coupon-pipeline-v1\",\n",
    "    name=\"coupon-model-training-pipeline\",\n",
    ")\n",
    "def pipeline(\n",
    "    project: str = \"nl-experiments\",\n",
    "    region: str = \"us-central1\"\n",
    "):\n",
    "    max_depth = 5\n",
    "    learning_rate = 0.2\n",
    "    n_estimators = 40\n",
    "\n",
    "    file_name = \"gs://bucket-name/in-vehicle-coupon-recommendation.csv\"\n",
    "    input_validation_task = validate_input_ds(filename=file_name)\n",
    "\n",
    "    with dsl.Condition(input_validation_task.outputs[\"input_validation\"] == \"true\"):\n",
    "        model_training_task = custom_training_job_component(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "        )\n",
    "\n",
    "        # Ensure that the custom training job depends on the input validation task\n",
    "        model_training_task.after(input_validation_task)\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='coupon-pipeline.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04a5e4-0bc2-4fb3-98f5-4eee6ef0b944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the pipeline job\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"simple-kubeflow-pipeline\",\n",
    "    template_path=\"coupon-pipeline.json\",\n",
    "    enable_caching=False,\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "start_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e9a68-6c35-4215-8dff-7522a4d894e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
